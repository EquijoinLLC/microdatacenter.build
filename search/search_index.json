{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Micro Data Center Community","text":""},{"location":"#what-is-a-micro-datacenter","title":"What is a micro datacenter?","text":"<p>This is largely Rob Gil's opinion if you're looking for someone to blame, but for the purposes of this project, we'll define it as the following.</p> <p>A micro datacenter is a fully functioning, ultra low power, and distributed datacenter. It contains all the functionality of a real datacenter, but small enough that you can run it off a 20A 110V outlet.</p> <p>Also</p> <p>it's bare metal...</p> <p>Why do I need a datacenter?</p> <p>As more and more services increase their costs, build walled gardens of content, and exploit our information for profit, many are looking to move to an alternative they can trust, themselves. It isn't necessarily easy to know how to do this in a time and cost efficient manner, especially in an eco-friendly matter. But it does gain you a lot of freedoms. For many of us, it is even fun!</p> <ul> <li>Privacy and the ability to migrate off of large centralized services to minimize your footprint in surveillance capitalism.</li> <li>Lower chatty traffic between the devices on your network and them phoning home.</li> <li>Lower your monthly subscription spend.</li> <li>Own your digital content.</li> <li>Run services to support your own business.</li> <li>Small business networking.</li> <li>Run AI on your data without training someone else's.</li> </ul> <p>Sure, but what is the maintenance cost? What if I have a power outage or a hardware mailfunction?</p> <p>This is where having a community is valuable that goes beyond just sharing some quick tips on installation, but also shares design and strategy to mitigate issues like data loss and down time without dedicating yourself to a life of on call duty. When you inevitibly do face an outage, you have a lot of folks hanging around who have experienced much of the same, so they can help dig you out while you're in panic mode, and then cover how to avoid that same issue moving forward. </p> <p>Many of us starting this community have experience with how large corporations ensure that consumers rarely experience downtime and fortunately for you, they layed a bunch of us off, so we are ready to share this wisdom with many of you! Who knows, as we grow and develop intertwining systems as a community, we may be able to build networks akin to the ipfs that use trusted freinds to hold copies of your data if something goes terribly wrong. </p> <p>Why a microdatacenter?</p> <p>Most folks, myself included, have always run some form of lab at home. This comes with a host of challenges and annoyances. - Enormous power requirements for anything more than 1/2 a rack of servers - Expensive, power hungry servers - Old servers that are out of support, clunky, expensive to fix and upgrade, etc - Overloading those servers with VMs - Many folks run VMWare ESX, but that can be pricey - Upgrades? What's that?    - Who actually has the funds to regularly ugprade their lab?</p>"},{"location":"#upcoming-developements","title":"Upcoming developements","text":"<p>This site is under heavy development, but first, why yet another homelab/selfhosted community?</p> <p>Our goal is to do a better job at consolidating the tutorials, mental models, and language in the amazing and existing communities (r/homelab, r/minilab, r/selfhosted, r/HomeServer, openminilabs, mini-rack, r/HomeNetworking) index those communities, and serve as the most ideal way to get started in this potentially large and complicated space of hosting your own services.</p> <p>Some nearterm goals:</p> <ol> <li>Simplify the existing \"single\" design into a getting started design that showcases a relatively cheap and affordable build, but starts to modularize the focus of various guides that build on the simple getting started guide.</li> <li>Break out guides into different taxonomies (Hardware:Motherboard/Memory/Disk/CPU/GPUs/TPUs/Thermal/Network/Power/Disks/Rack/etc.., Ops:VMs/Containers/Managers/Hypervisor/Configuration/KVM/Netboot/etc.., Software:Storage/Git/Forejo/Certbot/ Personal Software: Nextcloud/Immich/Monero/)</li> <li>Create a Recommendations index for technologies, people, and other stuff to watch and over time provide the ability for community driven benchmarks or showcases.</li> <li>Build out a Discord Forum to discuss topics in a more managed way as oppossed to the Reddit design.</li> <li>Recycle guides - initially, these guides focus on small, quiet, energy efficient builds using smaller micro servers like Raspberry Pi. This is not only pragmatic from a cost and energy savings, but supports smaller chip and dev board manufactures (especially built on open instruction sets like RISC-V). However, there is another consideration as we transition to a smaller interconnected network of nodes, and that is what do we do with servers that will inevitibly start spinning down and heading towards the landfill for children in Ghana to dissassemble? Well, we will also talk about standard homelab transformation of the most common larger systems. Although these won't be energy efficient, we as a community can also look into ways of opitmizing the most power hungry boards to something a bit closer to enable their reuse. When boards are at their end of life, we can also start to look at distributed recycling guides.</li> </ol> <p>Some longterm goals:</p> <ol> <li>Compile and provide guides on basic and intermediate Linux commands so that anyone, not just software engineers, can self-educate on the necessary understanding of systems administration.</li> <li>Compile and provide guides on computer, network, nad security basics so that anyone hosting their services can avoid the most common pitfalls in their security model.</li> </ol>"},{"location":"design/","title":"Design","text":"<p>In this draft design, I'll lay out the rack design and hardware necessary to power dozens of servers. </p>"},{"location":"design/#bill-of-materials","title":"Bill of Materials","text":"<ul> <li>Mobile 1/2 cabinet (or 1/4 cabinet if you want to do a smaller version)</li> <li>2x 6U DIN Mount with cable management</li> <li>Raspberry Pi 4 CM4</li> <li>Raspberry Pi 4 Carrier Board</li> <li>POE Switch (Ex. NETGEAR 24 Port POE Switch)</li> <li>Raspberry Pi DIN Rail Mount</li> <li>NVMe 2242</li> <li>PoE USB C Splitter Be careful with PoE USB Splitters. Not all are created equal. The one I linked is 5v at 4 Amp. The Utronics is only 2.4 Amp. I've also had some of the Utronics fail on me, so it's worth it to get a beefier splitter, especially if you have more to power. </li> </ul>"},{"location":"design/#the-rack","title":"The Rack","text":"<p>Do you need to do 1/2 a cabinet?</p> <p>No. You could do this smaller.</p> <p>But for this example, I'll do 1/2 a cabinet with room for things like battery backup / UPS...</p> <ul> <li>1/2 Cabinet: $374.00</li> <li>6U DIN Rack/Cable Mgmt: $199.00</li> </ul> <p>Total: $574.00</p>"},{"location":"design/#the-basic-host","title":"The basic host","text":"<p>The basic host configuration is as follows - PoE Powered Raspberry Pi - NVMe Attached Disk - Vertically mounted on DIN rails - No case required - better airflow and cooling</p> <p>This ultra low profile design dispenses with the bulky and failure prone PoE HATs. The PoE hats often have fan failures and take up valuable HAT space for other things. The carrier board in this BOM has the NVMe on the bottom of the board. With this design, the disk and PoE do not take up any HAT space and also don't glog up the rack with USB cables. The NVMe is also attached with PCIe speed rather than USB. </p> <p>Cost - NVMe (512G): 69$ - Raspberry Pi 4 (8G): 75$ - Waveshare Carrier Board: 30$ - DIN Mount: 15$ - Network Port (Switch / Num Ports): $ 11.45 per port (1) - POE Splitter: 20$</p> <p>Total: $221.00 / node</p> <p>Could you get used gear for this cheap? Yes. Would it be more powerful? Yes. But... - It's bulky as hell (rackmount or old desktop) - Power hungry - Is not expandible - Doesn't grow past the built in number of SATA ports</p>"},{"location":"design/#density","title":"Density","text":"<p>With the above BOM (Bill of Materials) we'll be able to get roughly 18 servers in to 6U. This assumes 2\" width per server. If you choose to go with SSD instead of NVMe, it can be cheaper, but you'll need additional DIN mounts and it takes up more rack space. Also add on the cost of the USB cables. </p> <p>Basically you'd need a 24port switch for every 6U. </p> <p>So in 7U, you'd have the switch, and 18 servers, with cable management. </p> <p>In that 22U cabinet, you could fit 54 servers.... now we're talking.</p>"},{"location":"design/#ups","title":"UPS","text":"<p>Rack Mounted DIY Battery Packs</p> <p>Yes, we're going that deep. If you want to of course. </p> <p>Building batteries from old laptop and car cells is a way to learn how do design power capacity for your datacenter. This is important information if you ever need to design or scope out a cage at a datacenter. Building batteries is also just fun. You can get really creative here with batteries and use all kinds of cells. My personal interest is in batteries that won't catch fire or explode. I'd also prefer batteries that don't off-gas in my office/microdatacenter space. </p> <p>If you're not familiar with battery types, it's a great time to learn the difference. My personal favorite is Lithium-titanate batteries. These are used in things like weather stations where they are not accessible and have very high recharge cycles. They're basically 20+ year batteries. Their power density isn't as high, but they're safe and won't explode and have a much larger temperature range than typical lithium batteries. LiFePO4 are also safe batteries for this kind of thing. </p>"},{"location":"software-installation/","title":"Software Installation and Configuration","text":""},{"location":"software-installation/#server-management-and-imaging","title":"Server Management and Imaging","text":"<p>Tinkerbell is an automated DHCP, NETBOOT, and PXE physical host provisioning service. To keep costs down, you can run this on your desktop in VMs initially. You can also run it on some of your nodes if you have a multi purpose node (something like a shared DHCP, DNS, Tinkerbell, image store, etc)</p>"},{"location":"software-installation/#routing","title":"Routing","text":"<p>FRRouting is an Open Source routing stack. This is the stack I would recommend for any home lab. This can be run on any linux host and is substantially cheaper than buying or re-using some purpose build switch or router. The goal of this project is to learn the internals, so running FRR is a good way to familiarize yourself with networking protocols. </p> <p>As we delve in to more detail, FRR becoming increasingly important to learn cloud networking concepts. I'm not talking about simple concepts like IGWs or VPC Route Tables. I mean the underpinning of VPCs in general. In a later article, I'll describe how you can create your own VPC networking with eVPN and VXLAN using FRRouting. This is the basis for undestanding multi tenant network architecture (hint, it's all encapsulated in the datacenter). </p>"},{"location":"software-installation/#public-ip-space","title":"Public IP Space","text":"<p>So you want to learn IPv6? Well, you could get your own ASN and IP space for about $1000. - ASN: 500$ - /36 IPv6: 500$</p> <p>/36 is the smallest ARIN will allocate at the time of writing</p> <p>Seems a little steep for most home SREs. But imagine all the money you'll save in not buying rackmount servers or powering them. </p> <p>With the routing setup and OpenSwitch, you can advertise your IP space through a provider like Equinix Metal (ask me how I know). You'll be able to have your own publicly routable IPv6 space. You can also use AWS BYOIP and TransitGateway Connect</p> <p>If you're more adventurous, have a bottomless wallet, or already have IPv4 space, you can do that too. </p>"},{"location":"software-installation/#accessing-your-microdatacenter-no-static-addressing-from-your-home-isp","title":"Accessing Your Microdatacenter (no static addressing from your home ISP)","text":"<p>You can use either wireguard or IPSec up to an Equinix Metal host or to AWS (or any other provider that can handle BGP)</p>"},{"location":"software-installation/#elastic-ips","title":"Elastic IPs","text":"<p>Elastic IPs, which you may be familar with from AWS. What this really is is just a public IP that is NAT'd to an ENI/Private IP. How do you achieve this in your microdatacenter? We don't have fancy network cards that offload this, so in order to do this, it's as simple as running BGP on every node. </p> <p>For kubernetes workloads, we'll use Cilium to advertise the public IP (Elastic IP) via BGP to the core network and out to the internet (if you're doing public peering)</p>"},{"location":"software-installation/#firewall","title":"Firewall","text":"<p>Firewalls we'll keep simple to understand them. No, we're not going to use PFSense for these exercises. We'll do it the hard way for the purposes of learning. </p> <p>nftables, bpfilter, iptables, etc</p>"},{"location":"software-installation/#k8s","title":"k8s","text":"<p>Kubernetes has always been a bit of a struggle in home labs. It's annoying running every service and having enough infrastructure to do so. The balance I've found is utilizing k3s</p> <p>This has support for things like Cilium, which ties in nicely to the routing stack. </p>"},{"location":"software-installation/#distributed-storage","title":"Distributed Storage","text":"<p>How do you do persistent storage? What can you use for shared storage and object stores? </p> <p>minio is the simplest to manage and utilize. It has an S3 like API and is a lot less work than other tools like GlusterFS and Ceph. </p>"}]}